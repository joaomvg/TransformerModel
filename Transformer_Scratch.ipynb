{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage\n",
    "from tqdm import tqdm\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPyImage('FFN.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self,embed_dim,filter_dim, drop_rate):\n",
    "        super(FFN,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.layer_1=nn.Linear(embed_dim,filter_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        self.layer_2=nn.Linear(filter_dim,embed_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out=self.layer_1(x)\n",
    "        out=self.relu(out)\n",
    "        out=self.drop(out)\n",
    "        out=self.layer_2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,drop_rate,num_heads=2):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        self.num_heads=num_heads\n",
    "        \n",
    "        self.attention_size=embed_dim//num_heads\n",
    "        self.scale=1/np.sqrt(embed_dim//num_heads)\n",
    "        \n",
    "        self.Q = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        self.K = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        self.V = nn.Linear(embed_dim, num_heads * self.attention_size, bias=False)\n",
    "        \n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.layer_out=nn.Linear(num_heads * self.attention_size,embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self,Q,K,V, mask):\n",
    "        \n",
    "        batch_size=Q.size(0)\n",
    "        d_K=self.attention_size\n",
    "        d_V=self.attention_size\n",
    "        \n",
    "        q=self.Q(Q)\n",
    "        q=q.view(batch_size,-1,self.num_heads,d_K) # [batch_size,Q_len,heads,d_K]\n",
    "        k=self.K(K)\n",
    "        k=k.view(batch_size,-1,self.num_heads,d_K) # [batch_size,K_len,heads,d_K]\n",
    "        v=self.K(V)\n",
    "        v=v.view(batch_size,-1,self.num_heads,d_V) # [batch_size,K_len,heads,d_K]\n",
    "        \n",
    "        q=q.transpose(1,2) # [batch_size,heads, K_len,d_K]\n",
    "        k=k.transpose(1, 2).transpose(2, 3)\n",
    "        v=v.transpose(1,2) # [batch_size,heads, K_len,d_K]\n",
    "        \n",
    "        out=self.scale*torch.matmul(q,k) # [batch_size,heads,Q_len,K_len]\n",
    "        out=out.masked_fill(mask, float('-inf'))\n",
    "        out=torch.softmax(out,dim=3) #along K_len\n",
    "        out=self.drop(out)\n",
    "\n",
    "        out=torch.matmul(out,v) # [batch_size,heads,Q_len, d_V]\n",
    "        out=out.transpose(1,2).contiguous()\n",
    "        out=out.view(batch_size,-1,self.num_heads*d_V)\n",
    "        \n",
    "        out=self.layer_out(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim, filter_dim,num_heads,drop_rate):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        # feed forward network\n",
    "        self.layer_norm_ffn=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn=FFN(embed_dim,filter_dim,drop_rate)\n",
    "        self.drop_ffn=nn.Dropout(drop_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        y=self.layer_norm(x)\n",
    "        y=self.multi_head(y,y,y,mask)\n",
    "        y=self.drop(y)\n",
    "        y=y+x\n",
    "        \n",
    "        #ffn\n",
    "        \n",
    "        z=self.layer_norm_ffn(y)\n",
    "        z=self.ffn(z)\n",
    "        z=self.drop_ffn(z)\n",
    "        z=z+y\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,num_heads,drop_rate):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        self.drop_rate=drop_rate\n",
    "        \n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.enc_dec_layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        self.enc_dec_multi_head=MultiHeadAttention(embed_dim,drop_rate,num_heads)\n",
    "        self.enc_dec_drop=nn.Dropout(drop_rate)\n",
    "        \n",
    "        self.layer_norm_ffn=nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.ffn=FFN(embed_dim,filter_dim,drop_rate)\n",
    "        self.drop_ffn=nn.Dropout(drop_rate)\n",
    "        \n",
    "        def forward(self,x,encoded, mask_1,mask_2):\n",
    "            \n",
    "            y=self.layer_norm(x)\n",
    "            y=self.multi_head(y,y,y,mask_1)\n",
    "            y=self.drop(y)\n",
    "            \n",
    "            y=x+y\n",
    "            \n",
    "            if encoded is not None:\n",
    "                z=self.enc_dec_layer_norm(y)\n",
    "                z=self.enc_dec_multi_head(y,encoded,encoded,mask_2)\n",
    "                z=self.enc_dec_drop(z)\n",
    "                z=self.enc_dec_drop(z)\n",
    "                \n",
    "                z=z+y\n",
    "            \n",
    "            w=self.layer_norm_ffn(z)\n",
    "            w=self.ffn(w)\n",
    "            w=self.drop_ffn(w)\n",
    "            \n",
    "            w=w+z\n",
    "            \n",
    "            return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,drop_rate,n_layers,num_heads):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        encoders=[EncoderLayer(embed_dim,filter_dim,num_heads,drop_rate) for _ in range(n_layers)]\n",
    "        self.encoders=nn.ModuleList(encoders)\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        \n",
    "    def forward(self,x,mask):\n",
    "        out=x\n",
    "        for layer in self.encoders:\n",
    "            out=layer(out,mask)\n",
    "        return self.layer_norm(out)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,embed_dim,filter_dim,drop_rate,n_layers,num_heads):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        decoders=[DecoderLayer(embed_dim,filter_dim,num_heads,drop_rate) for _ in range(n_layers)]\n",
    "        self.decoders=nn.ModuleList(decoders)\n",
    "        self.layer_norm=nn.LayerNorm(embed_dim,eps=1e-6)\n",
    "        \n",
    "    def forward(self, targets, encoded, mask_1,mask_2):\n",
    "        out = targets\n",
    "        for dec_layer in self.decoders:\n",
    "            out = dec_layer(out, encoded, mask2, mask1)\n",
    "        return self.last_norm(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, dropout_rate=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        pe = torch.zeros(max_len, embed_dim) # [max_len,embed_dim]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        # use regiter_buffer so this tensor is not updated under training\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [:,embed_dim]\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # For text generation, input vocab= target vocab\n",
    "    def __init__(self,vocab_size,n_layers,embed_dim,filter_dim,num_heads,dropout_rate):\n",
    "        super(Transformer,self).__init__()\n",
    "            \n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.filter_dim=filter_dim\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size,embed_dim)\n",
    "        self.encoder=Encoder(embed_dim,filter_dim,dropout_rate,n_layers,num_heads)\n",
    "        self.decoder=nn.Linear(embed_dim,vocab_size)\n",
    "\n",
    "        self.pos=PositionalEncoding(embed_dim,dropout_rate)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        \n",
    "        x_embed=self.embedding(x)*math.sqrt(self.embed_dim)\n",
    "        x_pos=self.pos(x_embed)\n",
    "        x_encoded=self.encoder(x_pos,mask)\n",
    "        x_decoded=self.decoder(x_encoded)\n",
    "        \n",
    "        return x_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_filepath='.data/wikitext-2/wiki.test.tokens'\n",
    "valid_filepath='.data/wikitext-2/wiki.valid.tokens'\n",
    "train_filepath= '.data/wikitext-2/wiki.train.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36718lines [00:01, 20940.56lines/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer,iter(io.open(train_filepath,encoding=\"utf8\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(iter(io.open(train_filepath, encoding=\"utf8\")))\n",
    "val_data = data_process(iter(io.open(valid_filepath, encoding=\"utf8\")))\n",
    "test_data = data_process(iter(io.open(test_filepath, encoding=\"utf8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.stoi) # the size of vocabulary\n",
    "embed_dim = 200 # embedding dimension\n",
    "filter_dim = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "n_layers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "num_heads = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = Transformer(vocab_size,n_layers, embed_dim,filter_dim,num_heads,dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    \n",
    "    return ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,optimizer, train_data,epochs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        src_mask=generate_mask(bptt).to(device)\n",
    "        for i in progressbar(range(0, train_data.size(0) - 1, bptt)):\n",
    "            data, targets = get_batch(train_data, i)\n",
    "            optimizer.zero_grad()\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = generate_mask(data.size(0)).to(device)\n",
    "            output = model(data.T, src_mask)\n",
    "            loss = criterion(output.view(-1, vocab_size), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print('epoch: ',epoch,', Loss: ',total_loss/train_data.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 3.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 , Loss:  0.1915034749282067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1 , Loss:  0.19086674898931166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:35"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2 , Loss:  0.19033246433494497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:38"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3 , Loss:  0.18974748692545867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4 , Loss:  0.18913434315409783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (9 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:34"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5 , Loss:  0.18857894826618526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:37"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6 , Loss:  0.18805452684042895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:41"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7 , Loss:  0.18746501500027885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n",
      "  0% (8 of 2929) |                       | Elapsed Time: 0:00:00 ETA:   0:00:40"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8 , Loss:  0.18699186953741484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:38 Time:  0:00:38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9 , Loss:  0.18649346193981958\n"
     ]
    }
   ],
   "source": [
    "train(model,criterion,optimizer,train_data,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model,data_source):\n",
    "    model.eval()\n",
    "    \n",
    "    src_mask=generate_mask(bptt).to(device)\n",
    "    acc=0\n",
    "    n=0\n",
    "    for i in progressbar(range(0, data_source.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(data_source, i)\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = generate_mask(data.size(0)).to(device)\n",
    "        output=model(data.T, src_mask)\n",
    "        output=output.view(-1, vocab_size)\n",
    "        prob=F.softmax(output,dim=-1)\n",
    "        pred=prob.argmax(dim=-1)\n",
    "        acc+=(pred==targets).sum().item()\n",
    "        n+=data.size(0)\n",
    "    \n",
    "    n=n*20\n",
    "    print('accuracy: ',acc/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (2929 of 2929) |####################| Elapsed Time: 0:00:16 Time:  0:00:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.050020976019044275\n"
     ]
    }
   ],
   "source": [
    "eval_model(model,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.Tensor([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N/A% (0 of 2929) |                       | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "src_mask=generate_mask(bptt).to(device)\n",
    "acc=0\n",
    "n=0\n",
    "for i in progressbar(range(0, train_data.size(0) - 1, bptt)):\n",
    "    data, targets = get_batch(train_data, i)\n",
    "    if data.size(0) != bptt:\n",
    "        src_mask = generate_mask(data.size(0)).to(device)\n",
    "    output=model(data.T, src_mask)\n",
    "    output=output.view(-1, vocab_size)\n",
    "    prob=F.softmax(output,dim=-1)\n",
    "    pred=prob.argmax(dim=-1)\n",
    "    acc+=(pred==targets).sum().item()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
